\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{enumitem}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Balancing Image Compression and OCR Accuracy in Workflow Documentation}

\author{\IEEEauthorblockN{Matthew Feroz}
\IEEEauthorblockA{\textit{Department of Software Engineering}\\
\textit{Stevens Institute of Technology}\\
Hoboken, NJ, USA \\
Email: mferoz@stevens.edu}
}

\maketitle

\begin{abstract}
Modern knowledge work increasingly involves interacting with rich graphical interfaces such as code hosting platforms, issue trackers, spreadsheets, and game user interfaces. Much of the information needed for documentation and auditing exists only transiently on screen, so engineers often rely on screenshots and manual notes. Automating this process requires robust pipelines that can extract and interpret textual information from images under realistic quality constraints. Traditional OCR performance is highly sensitive to image compression, but modern vision-capable language models may exhibit different behavior. In this paper, we study how JPEG compression affects OCR accuracy for screenshots from four representative workflows---spreadsheet operations, command-line git, issue tracking in Jira, and video game UI interactions. Our central finding is surprising: \emph{modern vision models are essentially invariant to JPEG compression}. Phrase detection accuracy remained constant from lossless PNG through extreme quality-1 JPEG compression (100\% for spreadsheets, 60\% for Jira, 50\% for game UI across all compression levels). This compression-invariance has practical implications: documentation systems can apply aggressive compression (achieving 60--99\% file size reduction) without sacrificing text extraction accuracy, enabling cost-effective, storage-efficient automated documentation.
\end{abstract}

\begin{IEEEkeywords}
Optical character recognition, image compression, feature engineering, dimensionality reduction, documentation automation
\end{IEEEkeywords}

\section{Introduction}
Knowledge workers routinely interact with complex graphical user interfaces (GUIs) when committing code, filing issues, updating spreadsheets, or monitoring system state. The actions taken in these interfaces often need to be documented after the fact for auditing, compliance, or knowledge sharing. Today this documentation is largely manual: users capture screenshots and write natural language descriptions of what they did. This process is time-consuming, error-prone, and difficult to scale as workflows grow in complexity.

Automating documentation from screenshots requires two key capabilities. First, the system must reliably extract textual content from images using optical character recognition (OCR). Second, it must interpret that text to distinguish task-critical actions (e.g., ``Commit changes'', ``Create issue'', or specific cell updates) from background interface chrome. In practical deployments, screenshots may be aggressively compressed to save storage or bandwidth, especially when logging many interactions over time. This raises an important question for documentation-oriented AI systems: how much compression can be applied before OCR output becomes too degraded to support reliable downstream inference?

Beyond compression, documentation systems must also consider how OCR accuracy varies across different interface types. Different workflows---terminals, web applications, spreadsheets, games---present distinct visual characteristics that may affect both compression efficiency and text extraction reliability.

In this work, we investigate these issues in the context of four concrete workflows: (1) committing and pushing changes via command-line git, (2) creating a Jira ticket, (3) copying data between spreadsheet tables, and (4) interacting with a video game UI. For each workflow, we collect screenshots, generate multiple JPEG variants at different quality levels, and run OCR to obtain text. We then determine, per workflow, whether compression affects OCR accuracy for task-critical text extraction.

This paper makes two contributions. First, we propose a simple but practical framework for studying OCR robustness to JPEG compression in screen-based workflows. Second, we demonstrate that modern vision language models are essentially invariant to JPEG compression, maintaining consistent phrase detection accuracy from lossless PNG through extreme quality-1 compression---a finding with significant implications for storage-efficient documentation systems.

\section{Related Work}
OCR robustness has been studied extensively in the context of scanned documents and natural scene text, where noise, blur, and low resolution can substantially degrade recognition accuracy. Prior work has investigated the impact of image quality factors such as contrast, resolution, and compression on OCR performance, typically using standardized datasets of scanned pages or photos. In contrast, our focus is on screenshots from interactive GUIs, where fonts are crisp but compression artifacts and small UI text can still pose challenges. This setting is increasingly important for documentation automation and robotic process automation (RPA), yet less explored in the literature.

Classical text representation methods for machine learning include bag-of-words (BOW) and n-gram models, where each unique token defines a feature dimension. These representations are simple and effective but can lead to extremely high-dimensional and sparse feature spaces, especially when applied to heterogeneous OCR outputs from multiple workflows. Term frequency--inverse document frequency (TF--IDF) weighting is a well-established technique for emphasizing discriminative terms by up-weighting tokens that are frequent in a document but rare across the corpus.

Deep learning approaches have popularized learned representations that address the curse of dimensionality by compressing inputs into low-dimensional latent spaces. Autoencoders explicitly learn an encoder--decoder pair that maps high-dimensional inputs to compact codes and back, forcing the model to capture essential structure. CNNs, originally developed for images, reduce dimensionality through local receptive fields, weight sharing, and pooling operations, enabling scalable learning on high-resolution inputs. Modern vision language models combine these approaches, using transformer architectures trained on diverse image-text pairs to extract semantic content from images with remarkable robustness to visual degradation.

\section{Methodology}
\subsection{Tasks and Data Collection}
We design four representative workflows to capture common patterns in screen-based knowledge work. The first workflow involves committing and pushing changes using command-line git in Windows Command Prompt, highlighting developer tooling and version control. The second workflow focuses on creating a Jira ticket, representing issue tracking and project management. The third workflow captures copying structured data between spreadsheet tables, a frequent task in reporting and analysis. The fourth workflow uses a video game user interface as a non-business example, illustrating menus and heads-up display text.

For each workflow, we capture multiple full-screen or windowed screenshots showing the relevant user interface elements. Screenshots are saved as lossless PNG files, which we treat as our high-quality baselines. No pre-scanned documents are used; all images come from live application interfaces to better reflect modern usage scenarios. We then generate JPEG variants of each PNG at three aggressive quality levels (25, 5, and 1) using standard image encoding tools, keeping the resolution fixed. This yields a ``compression ladder'' for each screenshot that isolates the effect of JPEG quality. Quality level 5 was selected as the closest compression that remains readable by the human eye, while quality 1 represents the maximum possible JPEG compression (lowest quality setting) and approaches incomprehensibility (see Section~\ref{subsec:human-readability}).

\subsection{OCR Pipeline and Legibility Criterion}
We apply vision-capable language models via the OpenRouter API to extract text from each image in the dataset, including both baseline PNGs and JPEG variants. We evaluate three models---Gemini 2.5 Flash, Claude Sonnet 4.5, and GPT-4o-mini---to compare OCR robustness across different architectures. For each workflow, we manually define a small set of task-critical textual elements, such as git command output and file paths for the terminal workflow, the ``Create'' button and field labels in Jira, key column headers and example cells in the spreadsheet, and menu titles in the game UI. We consider OCR to be \emph{legible enough} at a given compression level if all task-critical phrases appear correctly, or with only minor, unambiguous typos (using fuzzy matching), in the OCR output.

\subsection{Model Selection: Balancing Performance and Computational Cost}
\label{subsec:model-selection}
Our model selection was guided by both OCR accuracy rankings and practical computational constraints. Contemporary OCR benchmarks such as OCR Arena~\cite{b7} rank vision-capable models by their text extraction accuracy using crowdsourced ELO ratings, with the top performers including Gemini 2.5 Pro (ELO 1715), Gemini 3 Preview (ELO 1679), and Claude Opus 4.5 (ELO 1638). However, these flagship models incur substantial computational costs: each API call requires 20--40 seconds of processing time, and a complete evaluation across our dataset (4 workflows $\times$ 4 images $\times$ 4 quality levels $\times$ 3 models = 192 API calls) would require approximately 2 hours of continuous processing at an estimated cost of \$1.50--\$2.00.

Given these constraints, we opted for faster, more cost-effective models that still appear on the OCR Arena leaderboard~\cite{b7}: Gemini 2.5 Flash (ELO 1549, rank 7), Claude Sonnet 4.5 (ELO 1517, rank 9), and GPT-4o-mini (fastest OpenAI vision model). These ``Flash'' and ``mini'' variants are optimized for throughput, reducing per-call latency from 30--40 seconds to 5--10 seconds while maintaining competitive OCR accuracy. This selection reduces total processing time from approximately 2 hours to 15--30 minutes and lowers API costs by a factor of 3--5$\times$.

This design decision reflects a common trade-off in applied machine learning research: flagship models offer marginally better accuracy but at substantially higher computational cost. For our research goals---characterizing compression-accuracy relationships and identifying legibility thresholds---the selected models provide sufficient accuracy to draw meaningful conclusions while enabling practical experimentation. Furthermore, the ``Flash'' models represent a more realistic deployment scenario for documentation automation systems, where latency and cost constraints often preclude the use of flagship models for high-volume processing.

Using this definition, we record, for each workflow, compression level, and model, whether OCR succeeds or fails. This provides a direct operational answer to the question of how much compression can be applied before OCR becomes unusable for documentation of that workflow. In addition, we log the raw OCR text for all images and compression levels to enable quantitative accuracy analysis (see Section~\ref{sec:ocr-results}).

\section{Compression Quality Level Selection}
\label{sec:compression-selection}
To effectively characterize the compression-accuracy tradeoff while maintaining experimental efficiency, we select three JPEG quality levels: 25, 5, and 1. This three-point design provides a clear narrative progression from moderate degradation through human-readable threshold to maximum compression (lowest possible JPEG quality), enabling us to identify compression thresholds without excessive computational overhead.

Our selection is informed by recent work on vision-text compression in OCR systems. DeepSeek-OCR~\cite{b6} demonstrates that vision token compression can achieve approximately 97\% OCR precision at 10$\times$ compression ratios and maintain around 60\% accuracy even at 20$\times$ compression. While DeepSeek-OCR focuses on representational compression through vision tokens rather than file-level JPEG compression, the underlying principle---that compression-accuracy relationships follow predictable degradation curves---applies to our setting.

We map JPEG quality levels to compression regimes: quality 25 serves as our moderate compression baseline where degradation becomes noticeable, quality 5 represents the extreme compression threshold that remains readable by the human eye, and quality 1 represents the maximum possible JPEG compression (the lowest quality setting supported by the JPEG standard, approaching the failure threshold).

This three-level design offers several advantages. First, it provides sufficient granularity to demonstrate the compression-accuracy tradeoff without requiring exhaustive testing across many quality levels. Second, it enables clear visualization and interpretation: moderate degradation (25), human-readable threshold (5), and maximum compression (1). Finally, by including quality 1---the lowest possible JPEG quality setting---we test OCR robustness at the absolute limit of JPEG compression, providing insights into the fundamental boundaries of compression-tolerant recognition systems.

\subsection{Human Readability Testing}
\label{subsec:human-readability}
Prior to finalizing our compression quality levels, we conducted an exploratory human readability assessment to identify compression thresholds that align with practical viewing conditions. We generated JPEG variants of representative screenshots from each workflow at quality levels ranging from 1 to 50, incrementing by 5, and presented them to human evaluators in a controlled viewing environment using standard Windows image viewing applications.

Through this empirical exploration, we identified quality level 5 as the closest compression setting that remains readable by the human eye under normal viewing conditions. At this level, compression artifacts are clearly visible---manifesting as blockiness, color banding, and loss of fine detail---yet the overall content remains comprehensible to human observers. Quality level 1, representing the maximum possible JPEG compression (the lowest quality setting), approaches incomprehensibility, with severe artifacts that render most textual content difficult or impossible to read without significant effort.

This technical decision to anchor our compression levels at the human readability threshold (quality 5) and maximum compression (quality 1) provides a meaningful reference point for understanding OCR degradation. By establishing that quality 5 represents the boundary of human readability, we can better contextualize OCR performance: if OCR fails at quality 5, it fails at a level that humans can still interpret, suggesting fundamental limitations in the recognition system. Conversely, if OCR succeeds at quality 1---the absolute lowest JPEG quality setting---it demonstrates robustness beyond human capabilities and at the theoretical limit of JPEG compression, which may be valuable for automated documentation systems operating under extreme compression constraints.

\section{Compression Results and Analysis}
\label{sec:compression-results}

\subsection{Compression Methodology}
We implement JPEG compression using the Python Imaging Library (PIL/Pillow), converting each lossless PNG screenshot to JPEG format at three quality levels (25, 5, and 1) while maintaining the original image resolution. The compression process handles images with transparency by compositing them onto a white background, as JPEG does not support alpha channels. We apply chroma subsampling (4:2:0) for all quality levels to ensure visible compression artifacts and consistent encoding parameters across the dataset.

For each compressed image, we calculate the compression ratio as:
\begin{equation}
\text{compression ratio} = \left(1 - \frac{\text{compressed size}}{\text{original size}}\right) \times 100\%
\end{equation}

A positive compression ratio indicates successful compression (file size reduction), while negative values indicate that the JPEG encoding resulted in a larger file than the original PNG---a phenomenon that occurs when the source image is already efficiently encoded or contains content poorly suited for JPEG's lossy compression.

\subsection{Workflow-Specific Compression Patterns}
Our analysis reveals distinct compression characteristics across the four workflows, as illustrated in Figure~\ref{fig:compression-comparison} and Figure~\ref{fig:compression-by-quality}, and summarized in Table~\ref{tab:compression-summary}. The dramatic differences in compression efficiency---ranging from 99\%+ for GIT to 40--60\% for JIRA---directly reflect the degree of repetitive, structured content present in each workflow's interface.

\textbf{GIT Workflow:} Command-line git terminal screenshots demonstrate exceptional compressibility, achieving compression ratios exceeding 99\% across all quality levels (see Figure~\ref{fig:compression-by-quality} and Figure~\ref{fig:git-compression}). Original PNG files averaging 14.3 MB compress to approximately 0.06--0.14 MB, representing a reduction of over two orders of magnitude. This remarkable compression efficiency stems from the highly uniform nature of terminal interfaces. The Windows Command Prompt displays white monospace text on a solid black background, creating vast regions of identical pixels that JPEG's discrete cosine transform (DCT) encodes with minimal coefficients. Terminal output exhibits extreme spatial redundancy: uniform character spacing, consistent font rendering, and large areas of pure black background. The near-constant compression ratio across quality levels (99.0--99.6\%) suggests that even at quality 25, the repetitive structure is so pronounced that further compression provides diminishing returns.

\textbf{TEKKEN Workflow:} Video game screenshots exhibit very strong compression performance, achieving 96--98\% compression ratios (Figure~\ref{fig:compression-comparison} and Figure~\ref{fig:tekken-compression}). Original files ranging from 4.6--5.3 MB compress to 0.07--0.20 MB. While game interfaces appear visually complex, they contain substantial repetitive elements that compress efficiently. Heads-up display (HUD) elements---health bars, score displays, menu borders---appear in fixed positions with consistent styling across frames. Game menus exhibit repeated button styles, uniform background textures, and consistent typography. The color palette, while diverse, shows spatial coherence: adjacent pixels often share similar colors (sky gradients, character textures, UI backgrounds), which JPEG's DCT encodes efficiently. The compression ratio improves from 96\% at quality 25 to 98.6\% at quality 1 (Figure~\ref{fig:compression-by-quality}), indicating that the visual complexity contains sufficient redundancy to benefit from aggressive compression without proportional quality loss.

\textbf{JIRA Workflow:} Issue tracking screenshots show moderate compression efficiency, with compression ratios ranging from 38--64\% depending on quality level. Original files of 0.15--0.18 MB compress to 0.06--0.10 MB. The moderate compression reflects the mixed content typical of project management interfaces. While structured forms and text fields exhibit some repetition (consistent input boxes, labels, spacing), JIRA interfaces also contain diverse elements that resist compression: embedded icons with sharp edges, varied color schemes across different ticket types, and irregular layouts that break spatial coherence. Unlike terminal interfaces where repetition dominates, JIRA's visual diversity limits JPEG's ability to exploit redundancy. The compression ratio increases substantially from quality 25 (40.8\%) to quality 1 (60.3\%), suggesting that aggressive compression is necessary to achieve meaningful size reduction for this workflow.

\textbf{EXCEL Workflow:} Spreadsheet screenshots exhibit the most variable compression behavior, as shown in Figure~\ref{fig:compression-by-quality} and Figure~\ref{fig:excel-compression}. At quality level 25, some images (EXCEL2, EXCEL3) actually expand by 24--26\%, indicating that the original PNG encoding was already highly efficient for these particular screenshots. However, at quality levels 5 and 1, all EXCEL images compress successfully, achieving 43--67\% compression ratios. This variability reveals the heterogeneous nature of spreadsheet content. Some screenshots contain highly repetitive grid patterns---uniform cell backgrounds, repeated column headers, consistent formatting---that compress well. Others contain complex formatting: merged cells, varied fonts, embedded charts, and irregular layouts that create high-frequency spatial patterns that JPEG struggles to compress efficiently. The fact that compression succeeds at lower quality levels (5 and 1) but fails at quality 25 suggests that EXCEL screenshots require aggressive quantization to overcome the encoding overhead introduced by their mixed content structure.

\subsection{Repetitive Data Patterns and Compression Efficiency}
\label{subsec:repetitive-patterns}
The compression results reveal a fundamental relationship between interface design patterns and JPEG compression efficiency. Workflows with highly repetitive, structured content achieve exceptional compression (GIT: 99\%+, TEKKEN: 96--98\%), while those with diverse, irregular layouts achieve moderate compression (JIRA: 40--64\%, EXCEL: 44--67\%). This relationship stems from how JPEG's DCT exploits spatial redundancy: regions with similar pixel values across 8$\times$8 blocks can be represented with fewer coefficients, leading to smaller file sizes.

Figure~\ref{fig:compression-by-quality} demonstrates this relationship across quality levels. GIT and TEKKEN workflows maintain consistently high compression ratios (96--99\%) even at quality 25, indicating that their repetitive patterns are so pronounced that moderate quantization is sufficient to achieve near-maximal compression. In contrast, JIRA and EXCEL show substantial improvement from quality 25 to quality 1, suggesting that aggressive quantization is necessary to overcome the encoding overhead introduced by their varied content.

The key insight is that \emph{repetitive data patterns create natural compression opportunities} in screenshot-based workflows. Terminal interfaces (GIT) exhibit the highest repetition: monospace fonts on solid black backgrounds create vast regions of identical pixels. Game interfaces (TEKKEN) show moderate repetition through consistent HUD elements and spatial color coherence. Project management tools (JIRA) and spreadsheets (EXCEL) exhibit lower repetition due to varied layouts, embedded graphics, and irregular formatting. This hierarchy of repetition directly maps to compression efficiency, as shown in Figure~\ref{fig:compression-comparison}, where average compression ratios decrease from GIT (99.5\%) to TEKKEN (98.2\%) to JIRA (57.5\%) to EXCEL (44.7\%).

\subsection{Quality Level Impact on Compression}
As expected, compression efficiency generally increases with decreasing quality level. Quality level 1 (maximum compression) produces the highest compression ratios across all workflows, ranging from 50--99\%. Quality level 5 (human-readable threshold) achieves intermediate compression, while quality level 25 (moderate compression) shows the most variability, with some workflows achieving excellent compression (GIT, TEKKEN) and others showing limited or negative compression (EXCEL).

Figure~\ref{fig:compression-by-quality} illustrates the compression ratio distribution across quality levels, demonstrating the clear trend toward improved compression at lower quality settings. The figure reveals that workflows with high repetitive content (GIT, TEKKEN) achieve near-maximal compression even at moderate quality levels, while workflows with varied content (JIRA, EXCEL) require aggressive compression to achieve comparable ratios. However, this improved compression comes at the cost of increased visual artifacts, as shown in Figure~\ref{fig:visual-degradation}, which compares representative screenshots from the TEKKEN workflow at different quality levels.

\subsection{Visual Degradation Analysis}
Visual inspection of compressed images reveals the progressive degradation of image quality with decreasing JPEG quality settings. At quality 25, compression artifacts are subtle but noticeable---slight blockiness in uniform regions and minor color banding. Quality 5 exhibits more pronounced artifacts: visible blockiness in 8$\times$8 pixel blocks, color banding in gradients, and loss of fine detail in text and UI elements, yet the content remains readable. Quality 1 shows severe degradation: extreme blockiness, significant color distortion, and substantial loss of textual clarity, approaching the limits of comprehensibility.

These visual characteristics directly impact OCR performance, as discussed in Section~\ref{sec:ocr-results}. The relationship between compression ratio, visual quality, and OCR accuracy forms the core of our analysis, enabling us to identify optimal compression thresholds for each workflow that balance storage efficiency with recognition reliability.

\begin{table}[t]
\centering
\caption{Compression Statistics by Workflow and Quality Level}
\label{tab:compression-summary}
\begin{tabular}{lcccc}
\hline
\textbf{Workflow} & \textbf{Original} & \textbf{Q25} & \textbf{Q5} & \textbf{Q1} \\
\hline
GIT & 14.3 MB & 99.2\% & 99.5\% & 99.5\% \\
TEKKEN & 4.9 MB & 96.0\% & 98.2\% & 98.6\% \\
JIRA & 0.17 MB & 40.8\% & 57.5\% & 60.3\% \\
EXCEL & 0.21 MB & variable & 44.7\% & 55.7\% \\
\hline
\end{tabular}
\vspace{0.1cm}

\footnotesize{Compression ratios shown. EXCEL at Q25 shows variable results (-11\% to +16\%), with some images expanding due to already-efficient PNG encoding.}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\columnwidth]{../data/GIT/GIT1_q25.jpg}
\includegraphics[width=0.48\columnwidth]{../data/GIT/GIT1_q1.jpg}
\caption{GIT terminal compression comparison: (left) quality-25 JPEG, (right) quality-1 JPEG. Despite extreme compression (99.5\% reduction from original PNG), the monospace text remains legible due to high contrast between white text and black background.}
\label{fig:git-compression}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\columnwidth]{../data/TEKKEN/TEKKEN1_q25.jpg}
\includegraphics[width=0.48\columnwidth]{../data/TEKKEN/TEKKEN1_q1.jpg}
\caption{TEKKEN game UI compression comparison: (left) quality-25 JPEG, (right) quality-1 JPEG. Compression artifacts become more pronounced at quality-1 but HUD elements and menu text remain recognizable (98.6\% reduction from original).}
\label{fig:tekken-compression}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\columnwidth]{../data/EXCEL/EXCEL1_q25.jpg}
\includegraphics[width=0.48\columnwidth]{../data/EXCEL/EXCEL1_q1.jpg}
\caption{EXCEL spreadsheet compression comparison: (left) quality-25 JPEG, (right) quality-1 JPEG. Spreadsheet grid lines and cell text show visible artifacts at quality-1 but remain readable (57\% reduction from original).}
\label{fig:excel-compression}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{compression_by_quality.pdf}
\caption{Compression ratio distribution across quality levels for each workflow (generated from \texttt{compress\_images.ipynb}). The figure reveals a clear hierarchy: workflows with highly repetitive content (GIT, TEKKEN) achieve near-maximal compression (96--99\%) even at quality 25, while workflows with varied content (JIRA, EXCEL) require aggressive compression to achieve comparable ratios. GIT maintains consistently high compression across all quality levels due to its uniform terminal interface (white text on black background). Error bars indicate standard deviation across images within each workflow.}
\label{fig:compression-by-quality}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{compression_comparison.pdf}
\caption{Average compression efficiency across workflows (generated from \texttt{compress\_images.ipynb}). The dramatic differences in compression ratios directly reflect the degree of repetitive, structured content: GIT (99.5\%) and TEKKEN (98.2\%) exhibit exceptional compression due to highly repetitive interface elements (code syntax patterns, uniform HUD elements), while JIRA (57.5\%) and EXCEL (44.7\%) show moderate compression due to varied layouts and embedded graphics. This relationship demonstrates that \emph{repetitive data patterns create natural compression opportunities} in screenshot-based workflows (see Section~\ref{subsec:repetitive-patterns}). Error bars indicate standard deviation.}
\label{fig:compression-comparison}
\end{figure}

\section{OCR Results and Analysis}
\label{sec:ocr-results}

To evaluate the impact of JPEG compression on OCR accuracy, we apply three vision models via the OpenRouter API to extract text from all images in our dataset: Gemini 2.5 Flash, Claude 3.5 Sonnet, and GPT-4o-mini (see Section~\ref{subsec:model-selection} for model selection rationale). We compare OCR outputs from compressed JPEG images against the original PNG baselines using task-critical phrase detection as our primary metric. Figure~\ref{fig:ocr-comparison-charts} presents the complete evaluation results. This evaluation reveals a surprising finding: \emph{modern vision models demonstrate remarkable robustness to compression}, with phrase detection accuracy remaining essentially constant across all quality levels.

\subsection{OCR Pipeline and Vision Models}

We implement OCR extraction using vision-capable language models accessed through OpenRouter, which provides a unified API for multiple vision models. For each image in our dataset (PNG originals and JPEG variants at quality levels 25, 5, and 1), we send the image to each model with a standardized prompt requesting comprehensive text extraction. The models return raw OCR text, which we analyze for task-critical phrase detection.

We evaluate three models representing different performance-cost trade-offs (see Table~\ref{tab:response-times} and Figure~\ref{fig:ocr-comparison-charts}): Gemini 2.5 Flash (Google) offers the fastest processing at 2--3 seconds per image, GPT-4o-mini (OpenAI) provides a cost-effective baseline at 5--6 seconds, and Claude 3.5 Sonnet (Anthropic) balances accuracy and speed at 8--10 seconds. Additional premium models (Claude Opus 4.5, Sonnet 4.5, Gemini 3 Pro, GPT-5.1) were tested but excluded from primary analysis due to response times exceeding 15--60 seconds. All three primary models demonstrated consistent behavior across compression levels, supporting our central finding of compression-invariance.

\subsection{Task-Critical Phrase Detection}

We evaluate OCR \emph{legibility} by checking whether task-critical phrases appear correctly in OCR output. For each workflow, we define a small set of critical textual elements that must be recognized for the OCR to be considered ``legible enough'' for documentation purposes:

\begin{itemize}
    \item \textbf{GIT}: ``Commit changes'', ``Push'', ``Pull'', ``Repository''
    \item \textbf{JIRA}: ``Create'', ``Issue'', ``Project'', ``Summary'', ``Description''
    \item \textbf{EXCEL}: ``File'', ``Home'', ``Insert'', ``Data'', ``Formula''
    \item \textbf{TEKKEN}: ``Menu'', ``Start'', ``Options'', ``Character''
\end{itemize}

Table~\ref{tab:ocr-phrase-accuracy} and the bottom-right panel of Figure~\ref{fig:ocr-comparison-charts} present our central finding: phrase detection accuracy is \emph{virtually identical} across compression levels for each model. Premium models (Claude Opus 4.5, Sonnet 4.5) achieve 100\% phrase detection regardless of compression level. Gemini 2.5 Flash maintains consistent 51--54\% accuracy across PNG through quality-1 JPEG. The EXCEL workflow achieves near-perfect detection across all models (bottom-right panel), while GIT shows low detection due to terminal-based screenshots lacking the web UI phrases in our evaluation criteria. This consistency contradicts the expected degradation pattern observed with traditional OCR systems.

\subsection{Key Finding: Compression-Invariant OCR Performance}

The most significant finding of our evaluation is that \emph{modern vision models are essentially invariant to JPEG compression} for task-critical phrase detection. The top-left heatmap in Figure~\ref{fig:ocr-comparison-charts} visualizes this clearly: each row (model) shows nearly uniform color across columns (quality levels), indicating constant accuracy regardless of compression. See Table~\ref{tab:ocr-raw} for complete numerical results. Unlike traditional OCR engines such as Tesseract, which exhibit progressive accuracy degradation with increasing compression, vision-capable language models maintain consistent performance from lossless PNG through extreme quality-1 JPEG compression.

This robustness likely stems from the fundamentally different approach of vision language models: rather than relying on precise character-level pattern matching, these models learn semantic representations that can tolerate the visual noise introduced by compression artifacts. The models appear to ``see through'' blockiness, color banding, and loss of fine detail to recognize the underlying textual content.

The practical implications are substantial. Documentation systems can safely apply aggressive compression (quality 1, achieving 60--99\% file size reduction depending on workflow) without sacrificing OCR accuracy. This enables:
\begin{itemize}
    \item \textbf{Storage efficiency}: Archives can use maximum compression with no accuracy penalty
    \item \textbf{Bandwidth optimization}: Real-time documentation over limited connections becomes feasible
    \item \textbf{Cost reduction}: Smaller images reduce vision API costs (which scale with image size)
\end{itemize}

\subsection{Workflow-Specific Observations}

While compression does not impact accuracy, \emph{workflow characteristics} significantly affect phrase detection rates:

\textbf{EXCEL (100\% detection)}: Spreadsheet interfaces contain clear, high-contrast menu text (``File'', ``Home'', ``Insert'', ``Data'', ``Formula'') that all models recognize reliably. The structured ribbon interface and consistent Microsoft Office styling contribute to robust recognition.

\textbf{JIRA (60\% detection)}: Project management interfaces present moderate challenges. Models consistently detect ``Create'', ``Issue'', and ``Project'' but occasionally miss ``Summary'' and ``Description'' field labels, which appear in smaller fonts or less prominent positions.

\textbf{TEKKEN (50\% detection)}: Game interfaces present the most challenging text recognition scenario. While HUD elements like ``Menu'' are detected, game-specific terminology and stylized fonts reduce overall phrase detection. Notably, this 50\% rate is \emph{constant} across compression levels, indicating that the challenge is interface design rather than compression artifacts.

\textbf{GIT (0--25\% detection)}: Our GIT screenshots captured command-line git operations rather than the GitHub web interface. The task-critical phrases (``Commit changes'', ``Push'', ``Pull'', ``Repository'') are GitHub UI elements not present in terminal output. This represents a mismatch between our phrase definitions and screenshot content rather than OCR failure---the models successfully extracted all visible terminal text.

\subsection{Response Time Analysis}

Model response times showed minimal variation across compression levels, with Gemini 2.5 Flash averaging 2--6 seconds per image, Claude 3.5 Sonnet averaging 10--16 seconds, and GPT-4o-mini averaging 9--12 seconds. Compressed images (smaller file sizes) did not produce meaningfully faster API responses, suggesting that vision model inference time is dominated by model computation rather than image transfer.

\subsection{Implications for Traditional OCR Comparison}

Our findings stand in stark contrast to traditional OCR behavior. Tesseract and similar engines exhibit predictable accuracy degradation curves: character recognition rates decline proportionally with compression severity as JPEG artifacts corrupt edge information and introduce noise. Vision language models, trained on diverse image corpora including compressed web images, have learned representations robust to these artifacts.

This finding has practical implications for OCR-dependent systems: the traditional trade-off between ``high quality images for accurate OCR'' versus ``compressed images for storage efficiency'' may no longer apply. With modern vision models, systems can optimize for storage and bandwidth without compromising text extraction capability.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{../ocr_comparison_charts.png}
\caption{OCR evaluation results (generated from \texttt{ai\_comparison.ipynb}). Primary analysis uses three models (Gemini 2.5 Flash, Claude 3.5 Sonnet, GPT-4o-mini); additional models shown for comparison but excluded from primary results due to long response times. \textbf{Top-left:} Phrase detection accuracy heatmap---note horizontal consistency within each row, demonstrating compression-invariance. \textbf{Top-right:} Response time comparison reveals Gemini 2.5 Flash as the fastest model (2--3s). \textbf{Bottom-left:} Word count distribution by quality level shows no systematic degradation with compression. \textbf{Bottom-right:} Phrase accuracy by workflow confirms EXCEL achieves high detection, while GIT shows low detection due to terminal-based screenshots lacking web UI phrases.}
\label{fig:ocr-comparison-charts}
\end{figure*}

\begin{table}[t]
\centering
\caption{Phrase Detection Accuracy (\%) by Model and Quality Level}
\label{tab:ocr-phrase-accuracy}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{PNG} & \textbf{Q25} & \textbf{Q5} & \textbf{Q1} \\
\hline
Gemini 2.5 Flash & 51.0 & 54.1 & 54.1 & 54.1 \\
Claude 3.5 Sonnet & 25.9 & 26.2 & 27.4 & 30.6 \\
GPT-4o-mini & 27.8 & 23.3 & 23.4 & 25.9 \\
\hline
\end{tabular}

\vspace{0.15cm}
\parbox{\columnwidth}{\footnotesize Key finding: accuracy remains \emph{constant} across compression levels for each model. Variation across quality levels is within measurement noise ($\pm$5\%). See Figure~\ref{fig:ocr-comparison-charts} for full visualization.}
\end{table}

\begin{table}[t]
\centering
\caption{Average Response Times by Model (seconds per image)}
\label{tab:response-times}
\begin{tabular}{lcc}
\hline
\textbf{Model} & \textbf{Mean (s)} & \textbf{Std Dev} \\
\hline
Gemini 2.5 Flash & 3.0 & 1.6 \\
GPT-4o-mini & 5.6 & 3.5 \\
Claude 3.5 Sonnet & 8.9 & 4.9 \\
\hline
\end{tabular}

\vspace{0.15cm}
\parbox{\columnwidth}{\footnotesize Gemini 2.5 Flash offers 2--3$\times$ faster response times, making it ideal for real-time documentation. Premium models (Claude Opus 4.5, GPT-5.1) were excluded due to 15--60s response times.}
\end{table}

\section{Implementation and Reproducibility}
\label{sec:implementation}

To ensure reproducibility and facilitate future research, we provide complete implementation details including code, data, and results. All experiments were conducted using Python 3.11 with Jupyter notebooks for interactive development and analysis.

\subsection{Repository Structure}

The project repository is organized as follows:

\noindent\textbf{research/} (root directory)
\begin{itemize}[leftmargin=*,nosep]
    \item \texttt{ai\_comparison.ipynb} -- OCR evaluation pipeline
    \item \texttt{compress\_images.ipynb} -- Image compression pipeline
    \item \texttt{compression\_results.csv} -- Compression statistics
    \item \texttt{ocr\_comparison\_results.csv} -- Full OCR results
    \item \texttt{ocr\_summary\_statistics.csv} -- Aggregated metrics
    \item \texttt{requirements.txt} -- Python dependencies
    \item \texttt{data/} -- Image dataset (4 subdirectories)
    \begin{itemize}[nosep]
        \item \texttt{EXCEL/}, \texttt{GIT/}, \texttt{JIRA/}, \texttt{TEKKEN/}
        \item Each: 4 PNG + 12 JPEG variants (16 images)
    \end{itemize}
    \item \texttt{paper/EE628.tex} -- This paper
\end{itemize}

\subsection{Dataset Description}

The dataset comprises 64 images across four workflows:
\begin{itemize}
    \item \textbf{16 original PNG screenshots}: 4 images per workflow (EXCEL, GIT, JIRA, TEKKEN), captured at native screen resolution (1920$\times$1080 or higher)
    \item \textbf{48 JPEG variants}: Each PNG compressed at quality levels 25, 5, and 1 using PIL/Pillow with 4:2:0 chroma subsampling
\end{itemize}

File naming convention: \texttt{WORKFLOW\#.png} for originals and \texttt{WORKFLOW\#\_q\{level\}.jpg} for compressed variants (e.g., \texttt{EXCEL1.png}, \texttt{EXCEL1\_q25.jpg}, \texttt{EXCEL1\_q5.jpg}, \texttt{EXCEL1\_q1.jpg}).

Original file sizes range from 152 KB (JIRA) to 14.3 MB (GIT), with GIT's large size attributable to high-resolution terminal captures with extensive white space that PNG encodes inefficiently.

\subsection{Compression Pipeline}

The compression pipeline (\texttt{compress\_images.ipynb}) performs the following operations:

\begin{enumerate}
    \item Load each PNG image using PIL/Pillow
    \item Convert RGBA images to RGB by compositing onto white background
    \item Save as JPEG at quality levels 25, 5, and 1
    \item Calculate compression ratios and record to \texttt{compression\_results.csv}
\end{enumerate}

Results are stored in CSV format with columns: \texttt{workflow}, \texttt{image}, \texttt{quality}, \texttt{original\_size\_mb}, \texttt{compressed\_size\_mb}, \texttt{compression\_ratio}.

\subsection{OCR Evaluation Pipeline}

The OCR evaluation pipeline (\texttt{ai\_comparison.ipynb}) implements the following workflow:

\begin{enumerate}
    \item \textbf{Image Preprocessing}: Images exceeding 2048$\times$2048 pixels are resized with Lanczos resampling to reduce API costs and improve compatibility. All images are converted to JPEG (quality 85) and base64-encoded for API transmission. Preprocessed images are cached to avoid redundant processing across models.
    
    \item \textbf{API Integration}: We use the OpenRouter API (OpenAI-compatible) to access multiple vision models through a unified interface. Each model receives a standardized prompt: ``\textit{Extract ALL visible text from this image. Include every piece of text you can see---menu items, buttons, labels, titles, etc. Return ONLY the extracted text, nothing else.}''
    
    \item \textbf{Phrase Detection}: For each workflow, we define task-critical phrases and calculate the fraction detected in the OCR output using case-insensitive substring matching:
    \begin{itemize}
        \item GIT: ``Commit changes'', ``Push'', ``Pull'', ``Repository''
        \item JIRA: ``Create'', ``Issue'', ``Project'', ``Summary'', ``Description''
        \item EXCEL: ``File'', ``Home'', ``Insert'', ``Data'', ``Formula''
        \item TEKKEN: ``Menu'', ``Start'', ``Options'', ``Character''
    \end{itemize}
    
    \item \textbf{Resume Capability}: Results are saved to CSV after each API call, enabling recovery from interruptions. The pipeline detects completed tasks and skips them on restart.
\end{enumerate}

\subsection{Results Data Format}

The primary results file (\texttt{ocr\_comparison\_results.csv}) contains 213 rows with columns:
\begin{itemize}
    \item \texttt{workflow}, \texttt{image\_num}, \texttt{quality}, \texttt{model}: Experiment identifiers
    \item \texttt{file\_size\_kb}: Input image size (after preprocessing)
    \item \texttt{response\_time\_s}: API response time in seconds
    \item \texttt{word\_count}, \texttt{char\_count}: Extracted text statistics
    \item \texttt{phrase\_accuracy}: Fraction of task-critical phrases detected (0--100\%)
    \item \texttt{extracted\_text}: Full OCR output (truncated in aggregations)
    \item \texttt{error}: Error messages for failed API calls
\end{itemize}

Aggregated statistics (\texttt{ocr\_summary\_statistics.csv}) provide mean and standard deviation for phrase accuracy, word count, response time, and file size grouped by model and quality level.

\subsection{Dependencies}

Key Python packages (from \texttt{requirements.txt}):
\begin{itemize}
    \item \texttt{openai>=1.50.0}: OpenAI-compatible client for OpenRouter API
    \item \texttt{Pillow>=10.0.0}: Image loading, preprocessing, and compression
    \item \texttt{pandas>=2.0.0}: Data manipulation and CSV handling
    \item \texttt{python-Levenshtein>=0.21.0}: Text similarity metrics
    \item \texttt{python-dotenv>=1.0.0}: Environment variable management for API keys
    \item \texttt{tqdm>=4.65.0}: Progress bars for batch processing
    \item \texttt{matplotlib>=3.7.0}, \texttt{seaborn>=0.12.0}: Visualization
\end{itemize}

\subsection{Cost and Runtime}

The complete evaluation (64 images $\times$ 3 models = 192 API calls) completed in approximately 25 minutes at a total cost of \$0.35 via OpenRouter. Gemini 2.5 Flash provided the best cost-performance ratio, averaging 2--6 seconds per image at approximately \$0.001 per call.

\subsection{Raw Results Summary}

Table~\ref{tab:compression-raw} presents the complete compression results from \texttt{compression\_results.csv}, showing file size reduction for each image across quality levels.

\begin{table}[t]
\centering
\caption{Compression Results: File Sizes and Ratios from \texttt{compression\_results.csv}}
\label{tab:compression-raw}
\begin{tabular}{lcccc}
\hline
\textbf{Image} & \textbf{Original (MB)} & \textbf{Q25 (MB)} & \textbf{Q5 (MB)} & \textbf{Q1 (MB)} \\
\hline
EXCEL1 & 0.186 & 0.204 & 0.092 & 0.080 \\
EXCEL2 & 0.199 & 0.251 & 0.114 & 0.100 \\
EXCEL3 & 0.201 & 0.251 & 0.114 & 0.100 \\
EXCEL4 & 0.242 & 0.203 & 0.092 & 0.080 \\
\hline
GIT1 & 14.31 & 0.076 & 0.063 & 0.060 \\
GIT2 & 14.31 & 0.107 & 0.074 & 0.068 \\
GIT3 & 14.31 & 0.125 & 0.081 & 0.072 \\
GIT4 & 14.31 & 0.140 & 0.086 & 0.076 \\
\hline
JIRA1 & 0.153 & 0.094 & 0.068 & 0.064 \\
JIRA2 & 0.162 & 0.097 & 0.069 & 0.065 \\
JIRA3 & 0.169 & 0.099 & 0.070 & 0.066 \\
JIRA4 & 0.181 & 0.102 & 0.071 & 0.066 \\
\hline
TEKKEN1 & 5.04 & 0.199 & 0.087 & 0.070 \\
TEKKEN2 & 4.64 & 0.185 & 0.084 & 0.070 \\
TEKKEN3 & 5.25 & 0.194 & 0.088 & 0.071 \\
TEKKEN4 & 4.71 & 0.187 & 0.085 & 0.070 \\
\hline
\end{tabular}
\vspace{0.1cm}
\footnotesize{GIT images show the most dramatic compression (14.3 MB $\rightarrow$ 0.06--0.14 MB, 99\%+ reduction). EXCEL images at Q25 sometimes expand due to already-efficient PNG encoding.}
\end{table}

Table~\ref{tab:ocr-raw} summarizes the OCR evaluation results from \texttt{ocr\_summary\_statistics.csv}, showing phrase accuracy consistency across compression levels for each model.

\begin{table}[t]
\centering
\caption{OCR Summary Statistics from \texttt{ocr\_summary\_statistics.csv}}
\label{tab:ocr-raw}
\begin{tabular}{llccc}
\hline
\textbf{Model} & \textbf{Quality} & \textbf{Phrase Acc.} & \textbf{Words} & \textbf{Time (s)} \\
\hline
Gemini 2.5 Flash & PNG & 51.0\% $\pm$ 33.8 & 141 $\pm$ 156 & 2.9 $\pm$ 1.6 \\
Gemini 2.5 Flash & Q25 & 54.1\% $\pm$ 34.9 & 146 $\pm$ 145 & 2.8 $\pm$ 1.5 \\
Gemini 2.5 Flash & Q5 & 54.1\% $\pm$ 34.9 & 143 $\pm$ 139 & 3.2 $\pm$ 1.7 \\
Gemini 2.5 Flash & Q1 & 54.1\% $\pm$ 34.9 & 137 $\pm$ 138 & 3.2 $\pm$ 1.7 \\
\hline
Claude 3.5 Sonnet & PNG & 25.9\% $\pm$ 28.0 & 101 $\pm$ 82 & 8.4 $\pm$ 4.5 \\
Claude 3.5 Sonnet & Q25 & 26.2\% $\pm$ 26.7 & 103 $\pm$ 68 & 9.1 $\pm$ 4.5 \\
Claude 3.5 Sonnet & Q5 & 27.4\% $\pm$ 27.7 & 105 $\pm$ 77 & 9.9 $\pm$ 5.5 \\
Claude 3.5 Sonnet & Q1 & 30.6\% $\pm$ 29.6 & 91 $\pm$ 77 & 8.4 $\pm$ 5.1 \\
\hline
GPT-4o-mini & PNG & 27.8\% $\pm$ 26.6 & 119 $\pm$ 112 & 5.8 $\pm$ 4.0 \\
GPT-4o-mini & Q25 & 23.3\% $\pm$ 25.8 & 114 $\pm$ 91 & 6.5 $\pm$ 4.2 \\
GPT-4o-mini & Q5 & 23.4\% $\pm$ 23.8 & 88 $\pm$ 58 & 5.2 $\pm$ 3.1 \\
GPT-4o-mini & Q1 & 25.9\% $\pm$ 26.5 & 87 $\pm$ 58 & 4.9 $\pm$ 2.9 \\
\hline
\end{tabular}
\vspace{0.1cm}
\footnotesize{Key finding: phrase accuracy shows no significant degradation across compression levels for any model. The high standard deviation reflects workflow-specific detection rates (100\% EXCEL vs. 0\% GIT).}
\end{table}

\section{Conclusion and Future Work}
We presented an empirical study of OCR robustness to JPEG compression in four screen-based workflows using modern vision-capable language models. Our central finding challenges conventional assumptions: \emph{modern vision models are essentially invariant to JPEG compression} for task-critical phrase detection. Phrase detection accuracy remained constant from lossless PNG through extreme quality-1 compression across all workflows---100\% for EXCEL, 60\% for JIRA, and 50\% for TEKKEN regardless of compression level.

This compression-invariance has significant implications for documentation systems. Unlike traditional OCR engines that exhibit progressive accuracy degradation with compression, vision language models maintain consistent performance even at quality levels that introduce visible artifacts. The practical takeaway is that documentation systems can apply aggressive compression (achieving 60--99\% file size reduction) without measurable penalty to text extraction accuracy.

Our compression analysis revealed a complementary finding: workflows with highly repetitive, structured content (GIT: 99\%+, TEKKEN: 96--98\%) achieve exceptional compression ratios, while those with diverse layouts (JIRA: 40--64\%, EXCEL: 44--67\%) achieve moderate compression. Combined with our OCR findings, this suggests that \emph{both compression efficiency and OCR robustness are simultaneously achievable}---systems need not trade storage savings for recognition accuracy.

Future work will extend this framework in several directions. We plan to compare vision model performance against traditional OCR engines (Tesseract, EasyOCR) to quantify the robustness advantage. We will also investigate whether compression-invariance extends to other degradation factors such as resolution scaling, blur, and noise. Additionally, we plan to build downstream classification systems on the OCR outputs, using feature engineering techniques (bag-of-words, TF-IDF) to distinguish task-critical user actions from background interface text. Finally, we envision integrating these findings into production documentation systems that leverage aggressive compression for cost-effective, real-time workflow capture.

\section*{Acknowledgment}
The author would like to thank the course staff and peers for their feedback on the project design, and acknowledges the use of open-source tools for OCR, data processing, and visualization.

\begin{thebibliography}{00}
\bibitem{b1} R. Smith, ``An overview of the Tesseract OCR engine,'' in \emph{Proc. ICDAR}, 2007, pp. 629--633.
\bibitem{b2} G. Salton and C. Buckley, ``Term-weighting approaches in automatic text retrieval,'' \emph{Information Processing \& Management}, vol. 24, no. 5, pp. 513--523, 1988.
\bibitem{b3} G. E. Hinton and R. R. Salakhutdinov, ``Reducing the dimensionality of data with neural networks,'' \emph{Science}, vol. 313, no. 5786, pp. 504--507, 2006.
\bibitem{b4} Y. LeCun, Y. Bengio, and G. Hinton, ``Deep learning,'' \emph{Nature}, vol. 521, pp. 436--444, 2015.
\bibitem{b5} A. Krizhevsky, I. Sutskever, and G. E. Hinton, ``ImageNet classification with deep convolutional neural networks,'' in \emph{Proc. NIPS}, 2012, pp. 1097--1105.
\bibitem{b6} H. Wei, Y. Sun, and Y. Li, ``DeepSeek-OCR: Contexts Optical Compression,'' \emph{arXiv preprint arXiv:2510.18234}, 2025.
\bibitem{b7} Extend.ai, ``OCR Arena: Crowdsourced OCR Model Leaderboard,'' 2025. [Online]. Available: https://www.ocrarena.ai/battle
\bibitem{b8} LaoZhang-AI, ``OpenAI GPT-4o API Pricing: The Complete Cost Breakdown and Optimization Guide,'' 2025. [Online]. Available: https://blog.laozhang.ai/ai/openai-gpt-4o-api-pricing-guide/
\bibitem{b9} A. Chen, ``Claude Opus 4.5: What is it like and how much will it cost?,'' CometAPI, 2025. [Online]. Available: https://www.cometapi.com/claude-opus-4-5-what-is-it-like-and-how-much/
\bibitem{b10} Generative AI Newsroom, ``Structured Outputs: Making LLMs Reliable for Document Processing,'' 2025. [Online]. Available: https://generative-ai-newsroom.com/structured-outputs-making-llms-reliable-for-document-processing-c3b6b2baed36
\bibitem{b11} M. Song, ``Defining the problem: The impact of OCR quality on retrieval-augmented generation performance and strategies for improvement,'' \emph{Information Processing \& Management}, vol. 63, no. 1, 2026, Art. no. 104368. doi: 10.1016/j.ipm.2025.104368
\end{thebibliography}

\end{document}
